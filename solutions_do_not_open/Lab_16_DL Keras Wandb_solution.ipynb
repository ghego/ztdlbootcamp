{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_t, y_train), (X_test_t, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train_t = X_train_t.astype('float32') / 255.\n",
    "X_test_t = X_test_t.astype('float32') / 255.\n",
    "\n",
    "X_train = X_train_t.reshape(len(X_train_t), 32*32*3)\n",
    "X_test = X_test_t.reshape(len(X_test_t), 32*32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set:\")\n",
    "print(\"Tensor images shape:\\t\", X_train_t.shape)\n",
    "print(\"Flat images shape:\\t\", X_train.shape)\n",
    "print(\"Labels shape:\\t\\t\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "for i in range(0, 8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    plt.imshow(X_train[i].reshape(32, 32, 3))\n",
    "    plt.title(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init()\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "config.layer_1_size  = 1024\n",
    "config.layer_2_size = 512\n",
    "config.dropout = 0.2\n",
    "config.learn_rate = 0.001\n",
    "config.epochs = 10\n",
    "config.batch_size=256\n",
    "config.activation='relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(config.layer_1_size,\n",
    "                activation=config.activation,\n",
    "                input_dim=3072))\n",
    "\n",
    "model.add(Dropout(config.dropout))\n",
    "\n",
    "model.add(Dense(config.layer_2_size,\n",
    "                activation=config.activation))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=config.learn_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with wandb.monitor():\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=config.batch_size,\n",
    "              epochs=config.epochs,\n",
    "              verbose=1,\n",
    "              validation_split=0.1,\n",
    "              callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Change the configuration parameters and re-run the training loop. Go to your Weights and Biases dashboard to compare the different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "empty"
    ]
   },
   "outputs": [],
   "source": [
    "wandb.init()\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "config.layer_1_size  = 1024\n",
    "config.layer_2_size = 512\n",
    "config.dropout = 0.2\n",
    "config.learn_rate = 0.01\n",
    "config.epochs = 10\n",
    "config.batch_size=512\n",
    "config.activation='relu'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(config.layer_1_size,\n",
    "                activation=config.activation,\n",
    "                input_dim=3072))\n",
    "\n",
    "model.add(Dropout(config.dropout))\n",
    "\n",
    "model.add(Dense(config.layer_2_size,\n",
    "                activation=config.activation))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=config.learn_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "with wandb.monitor():\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=config.batch_size,\n",
    "              epochs=config.epochs,\n",
    "              verbose=1,\n",
    "              validation_split=0.1,\n",
    "              callbacks=[WandbCallback()])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
